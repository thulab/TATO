{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TATO Tutorial: Basic Time Series Forecasting\n",
    "\n",
    "This tutorial demonstrates the basic usage of TATO (Adaptive Transformation Optimization) for time series forecasting.\n",
    "\n",
    "## Overview\n",
    "\n",
    "TATO is a framework that automatically optimizes data transformations to enhance the performance of time series foundation models. In this tutorial, we'll:\n",
    "\n",
    "1. Load a time series dataset\n",
    "2. Initialize a foundation model\n",
    "3. Configure and apply transformations\n",
    "4. Run adaptive optimization\n",
    "5. Evaluate results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have TATO installed and the required datasets downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('.'))))\n",
    "\n",
    "# Import TATO modules\n",
    "from data.dataset import get_dataset, CustomDataset\n",
    "from model.model_factory import ModelFactory\n",
    "from transformation.transformation_factory import TransformationFactory\n",
    "from pipeline.base import BasePipeline\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.metrics import set_seed\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset\n",
    "\n",
    "TATO supports multiple benchmark datasets. Let's load the ETTh1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_name = 'ETTh1'\n",
    "dataset = get_dataset(dataset_name, fast_split=True)  # fast_split=True for quick testing\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Training samples: {dataset.train_len}\")\n",
    "print(f\"Validation samples: {dataset.val_len}\")\n",
    "print(f\"Test samples: {dataset.test_len}\")\n",
    "print(f\"Feature dimension: {dataset.feature_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualize Data\n",
    "\n",
    "Let's visualize the time series data to understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for visualization\n",
    "train_data = dataset.np_data_dict['OT'][dataset.train_start:dataset.train_end]\n",
    "val_data = dataset.np_data_dict['OT'][dataset.val_start:dataset.val_end]\n",
    "test_data = dataset.np_data_dict['OT'][dataset.test_start:dataset.test_end]\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "\n",
    "axes[0].plot(train_data, label='Training Data', color='blue', alpha=0.7)\n",
    "axes[0].set_title('Training Set')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(val_data, label='Validation Data', color='green', alpha=0.7)\n",
    "axes[1].set_title('Validation Set')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(test_data, label='Test Data', color='red', alpha=0.7)\n",
    "axes[2].set_title('Test Set')\n",
    "axes[2].set_xlabel('Time')\n",
    "axes[2].set_ylabel('Value')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Model\n",
    "\n",
    "TATO supports multiple foundation models. Let's load the Timer-LOTSA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model_name = 'Timer-LOTSA'\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = ModelFactory.load_model(\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    args=None\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model patch length: {model.patch_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Data Loader\n",
    "\n",
    "Prepare the data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "pred_len = 96  # Predict next 96 time steps\n",
    "max_seq_len = 1440  # Use 1440 historical points\n",
    "batch_size = 32\n",
    "num_samples = 100  # Use 100 samples for quick demonstration\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    dataset=dataset,\n",
    "    mode='train',\n",
    "    target_column='OT',\n",
    "    max_seq_len=max_seq_len,\n",
    "    pred_len=pred_len,\n",
    "    augmentor=None,\n",
    "    num_sample=num_samples\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Dataloader created with {len(train_dataset)} samples\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Inspect one batch\n",
    "for real_idx, history, label in train_loader:\n",
    "    print(f\"Batch history shape: {history.shape}\")  # (batch_size, seq_len, 1)\n",
    "    print(f\"Batch label shape: {label.shape}\")      # (batch_size, pred_len, 1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure Transformations\n",
    "\n",
    "TATO provides various data transformations. Let's configure a simple transformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available transformations\n",
    "transformation_dict = TransformationFactory.get_transformation_dict()\n",
    "print(f\"Available transformations: {list(transformation_dict.keys())}\")\n",
    "\n",
    "# Create transformation instances\n",
    "transformations = [\n",
    "    transformation_dict['normalizer'](\n",
    "        method='standard',\n",
    "        mode='input'\n",
    "    ),\n",
    "    transformation_dict['trimmer'](\n",
    "        seq_l=10,\n",
    "        patch_len=96,\n",
    "        pred_len=pred_len\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Created {len(transformations)} transformations:\")\n",
    "for tf in transformations:\n",
    "    print(f\"  - {type(tf).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create and Test Pipeline\n",
    "\n",
    "Create a pipeline with the transformations and model, then test it on a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "config = {\n",
    "    'patch_len': 96,\n",
    "    'pred_len': pred_len,\n",
    "    'data_patch_len': 96,\n",
    "    'model_patch_len': 96\n",
    "}\n",
    "\n",
    "pipeline = BasePipeline(\n",
    "    transformations=transformations,\n",
    "    config=config,\n",
    "    model=model,\n",
    "    pred_len=pred_len,\n",
    "    augmentor=None,\n",
    "    mode='train'\n",
    ")\n",
    "\n",
    "# Test pipeline on a single batch\n",
    "for real_idx, history, label in train_loader:\n",
    "    # Convert to numpy for pipeline\n",
    "    history_np = history.numpy()\n",
    "    label_np = label.numpy()\n",
    "    \n",
    "    # Run pipeline\n",
    "    predictions, _ = pipeline.run_batch(history_np, label_np)\n",
    "    \n",
    "    print(f\"Input shape: {history_np.shape}\")\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    print(f\"Labels shape: {label_np.shape}\")\n",
    "    \n",
    "    # Calculate MSE for this batch\n",
    "    mse = np.mean((predictions - label_np) ** 2)\n",
    "    print(f\"Batch MSE: {mse:.6f}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Predictions\n",
    "\n",
    "Let's visualize the predictions for a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for first 3 samples\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 9))\n",
    "\n",
    "for i in range(3):\n",
    "    # Get data for sample i\n",
    "    sample_history = history_np[i].squeeze()\n",
    "    sample_pred = predictions[i].squeeze()\n",
    "    sample_label = label_np[i].squeeze()\n",
    "    \n",
    "    # Create time indices\n",
    "    history_time = np.arange(len(sample_history))\n",
    "    pred_time = np.arange(len(sample_history), len(sample_history) + len(sample_pred))\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].plot(history_time, sample_history, \n",
    "                label='History', color='blue', linewidth=2)\n",
    "    axes[i].plot(pred_time, sample_pred, \n",
    "                label='Prediction', color='red', linestyle='--', linewidth=2)\n",
    "    axes[i].plot(pred_time, sample_label, \n",
    "                label='Ground Truth', color='green', linestyle=':', linewidth=2)\n",
    "    \n",
    "    axes[i].set_title(f'Sample {i+1} Predictions')\n",
    "    axes[i].set_xlabel('Time Step')\n",
    "    axes[i].set_ylabel('Value')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add vertical line separating history and prediction\n",
    "    axes[i].axvline(x=len(sample_history), color='black', \n",
    "                   linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Full Pipeline\n",
    "\n",
    "Evaluate the pipeline on the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate pipeline\n",
    "print(\"Evaluating pipeline on training data...\")\n",
    "\n",
    "metrics = pipeline.evaluate(train_loader)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_metrics = {}\n",
    "for metric_name, values in metrics.items():\n",
    "    if len(values) > 0:\n",
    "        avg_metrics[metric_name] = np.mean(values)\n",
    "\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for metric_name, value in avg_metrics.items():\n",
    "    print(f\"  {metric_name}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Adaptive Transformation Search (Optional)\n",
    "\n",
    "For a more advanced demonstration, we can run adaptive transformation search. This requires more computational resources and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Full adaptive search is computationally intensive\n",
    "# Uncomment and run only if you have sufficient resources\n",
    "\n",
    "# from tuner.tuner_factory import TunerFactory\n",
    "# from pipeline.pipeline_factory import PipelineFactory\n",
    "# \n",
    "# # Define transformation search space\n",
    "# transformation_names = ['normalizer', 'sampler', 'trimmer']\n",
    "# distribution = TunerFactory.build_search_space(transformation_names, patch_len=96)\n",
    "# \n",
    "# # Run a few search trials\n",
    "# num_trials = 10\n",
    "# trial_results = []\n",
    "# \n",
    "# for trial_idx in range(num_trials):\n",
    "#     print(f\"Trial {trial_idx + 1}/{num_trials}\")\n",
    "#     \n",
    "#     # Sample parameters\n",
    "#     params = {}\n",
    "#     for key in distribution.keys():\n",
    "#         if hasattr(distribution[key], 'sample'):\n",
    "#             params[key] = distribution[key].sample()\n",
    "#         else:\n",
    "#             params[key] = np.random.choice(distribution[key])\n",
    "#     \n",
    "#     # Build transformations\n",
    "#     transformations = []\n",
    "#     if params.get('normalizer_method', 'none') != 'none':\n",
    "#         transformations.append(\n",
    "#             transformation_dict['normalizer'](\n",
    "#                 method=params['normalizer_method'],\n",
    "#                 mode=params.get('normalizer_mode', 'input')\n",
    "#             )\n",
    "#         )\n",
    "#     \n",
    "#     # Evaluate and store results\n",
    "#     # ... (similar to previous evaluation)\n",
    "# \n",
    "# print(\"Adaptive search completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've demonstrated:\n",
    "\n",
    "1. How to load and visualize time series data with TATO\n",
    "2. How to initialize foundation models\n",
